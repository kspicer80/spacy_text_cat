{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/96f5xt0s59j0cyt_3yf1j_0w0000gn/T/ipykernel_53470/1337929515.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import html as ihtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsons_into_dataframe(directory):\n",
    "    temp_list_of_dfs = []\n",
    "    directory = directory\n",
    "    pathlist = Path(directory).rglob('*.json')\n",
    "    for path in pathlist:\n",
    "        with open(path) as f:\n",
    "            json_data = pd.json_normalize(json.loads(f.read()))\n",
    "        temp_list_of_dfs.append(json_data)\n",
    "    combined_df = pd.concat(temp_list_of_dfs, ignore_index=True)\n",
    "    return(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1700s = read_jsons_into_dataframe('1700s')\n",
    "df_1800s = read_jsons_into_dataframe('1800s')\n",
    "df_1900s = read_jsons_into_dataframe('1900s')\n",
    "df_2000s = read_jsons_into_dataframe('2000s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1700s['label'] = 1700\n",
    "df_1800s['label'] = 1800\n",
    "df_1900s['label'] = 1900\n",
    "df_2000s['label'] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date_blocked', 'id', 'blocked', 'judges', 'court', 'date_filed',\n",
      "       'download_url', 'source', 'local_path', 'html_lawbox', 'time_retrieved',\n",
      "       'nature_of_suit', 'plain_text', 'html_with_citations', 'sha1',\n",
      "       'date_modified', 'precedential_status', 'extracted_by_ocr',\n",
      "       'citation_count', 'absolute_url', 'docket', 'html', 'resource_uri',\n",
      "       'citation.state_cite_three', 'citation.federal_cite_two',\n",
      "       'citation.resource_uri', 'citation.federal_cite_three',\n",
      "       'citation.lexis_cite', 'citation.document_uris',\n",
      "       'citation.scotus_early_cite', 'citation.federal_cite_one',\n",
      "       'citation.case_name', 'citation.westlaw_cite',\n",
      "       'citation.state_cite_one', 'citation.specialty_cite_one',\n",
      "       'citation.state_cite_regional', 'citation.id', 'citation.docket_number',\n",
      "       'citation.state_cite_two', 'citation.neutral_cite',\n",
      "       'supreme_court_db_id', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([df_1700s, df_1800s, df_1900s, df_2000s])\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63374, 5)\n",
      "(61314, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "merged_df_of_interest = merged_df[['date_filed', 'plain_text', 'html', 'html_with_citations', 'label']]\n",
    "merged_df_of_interest = merged_df_of_interest.replace(r'^\\s*$', np.nan, regex=True)\n",
    "print(merged_df_of_interest.shape)\n",
    "merged_df_of_interest = merged_df_of_interest.dropna(subset=['plain_text', 'html', 'html_with_citations'], how='all')\n",
    "print(merged_df_of_interest.shape)\n",
    "#filter = merged_df_of_interest['html'] != ''\n",
    "#filtered_df = df[filter]\n",
    "#filtered_df = filtered_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427\n"
     ]
    }
   ],
   "source": [
    "print(merged_df_of_interest['html'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_text(html_text):\n",
    "    if html_text == '':\n",
    "        pass \n",
    "    else:\n",
    "        cleaned_text = BeautifulSoup(html_text, 'lxml').get_text()\n",
    "    return(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(html_text):\n",
    "    if html_text == '':\n",
    "        pass\n",
    "    else:\n",
    "        text = BeautifulSoup(ihtml.unescape(html_text), \"lxml\").text\n",
    "        text = re.sub(r\"http[s]?://\\S+\", \"\", html_text)\n",
    "        text = re.sub(r\"\\s+\", \" \", html_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(html):\n",
    "    if html == '':\n",
    "        pass\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for data in soup(['style', 'script', 'code', 'a']):\n",
    "            data.decompose()\n",
    "    return ' '.join(soup.stripped_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = merged_df_of_interest.copy()\n",
    "#m_df.head()\n",
    "#print(m_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_parser(raw_html):\n",
    "    raw_html = str(raw_html)\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    soup_string = soup.get_text()\n",
    "    soup_string = re.sub('<.*>', ' ', soup_string)\n",
    "    return soup_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfa89dee88b4f0d8ad61889e5533d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_df['cleaned_text'] = m_df['html_with_citations'].progress_apply(lambda x: html_parser(x) if x != '' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df['cleaned_text'] = m_df['cleaned_text'].replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df.to_csv('cleaned_text_file.csv', sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e56c70b3071a7c9e5271d6c05d63446be4cb37f733ae995dda36f1f67e797e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
